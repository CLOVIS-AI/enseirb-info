\documentclass[a4paper,10pt,french,openany]{memoir}
\usepackage[utf8]{inputenc}
\usepackage{babel}

\usepackage{clovisai}

\newcommand{\absfrac}[2]{\frac{\left|#1\right|}{\left|#2\right|}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\normo}[1]{\norm{#1}_1}
\newcommand{\normfrac}[2]{\frac{\norm{#1}}{\norm{#2}}}
\DeclareMathOperator{\cond}{cond}
\lstset{language=Python}

%opening
\title{Probabilités et Statistiques}
\author{Notes prises par Ivan Canet}

\begin{document}

\maketitle
\tableofcontents

\chapter{Méthodes numériques}

\section{Représentation machine des nombres}

\subsection{Entiers}

Les entiers sont représentés en binaire sur $n$ bits où le premier est le signe; donc $n \in [-2^{n-1}, 2^{n-1}]$.

Les entiers usuels (type \lstinline{int} en C++, Java\dots) vont environ de -2 milliards à 2 milliards.

\paragraph{Complément à deux}
Pour représenter un entier négatif, on fait les opérations suivantes:
\begin{itemize}
 \item On calcule la représentation binaire de son inverse ($-p$),
 \item Opération miroir ($0 \rightarrow 1$, $1 \rightarrow 0$),
 \item On ajoute 1.
\end{itemize}

Par exemple, -17 devient 010001, 101110 puis 101111.

\subsection{Réels}

Les réels sont représentés sous la forme: \[ x = s y 2^e \] où $s$ est le bit de signe, $y$ est la mantisse et est incluse dans $[0.5, 1]$ et $e$ est l'exposant.

Pour les nombres en précision double (C, Python, Java\dots), $s$ est codé sur 1 bit, la mantisse sur 52 et l'exposant sur 11. Les nombres peuvent donc être écrits de $10^{-308}$ à $10^{308}$.

Pour calculer $y$:
\begin{itemize}
 \item Trouver l'exposant $e$ en multipliant (ou divisant) $x$ par des puissances de 2, tel que $0.5 \leq x 2^{-e} \leq 1$,
 \item Multiplier $x$ par 2 pour obtenir le bit suivant de la mantisse, on itère en ignorant la partie entière.
\end{itemize}

%TODO Exemple

\paragraph{Précision}
On appelle précision l'écart entre 1 et le nombre le plus proche représentable. Pour un nombre à simple précision il s'agit de $2.10^{-7}$, et $10^{-16}$ pour les nombres à double précision.

\section{Erreurs d'arrondis}

Un nombre ayant une écriture finie en décimal aura le plus souvent une représentation approchée en binaire.

\subsection{Erreurs lors de l'addition}

On additionne $x + \delta x$ et $y + \delta y$, on a donc $x + y + \delta x + \delta y$.

L'erreur relative est:
\begin{align*}
 \left|\frac{x+y-(x+y+\delta x+\delta y)}{x+y}\right| & = \left| \frac{\delta x + \delta y}{x+y} \right| \\
 & \leq \frac{\left|\delta x\right|}{\left|x+y\right|} + \frac{\left|\delta y\right|}{\left|x+y\right|}
\end{align*}
à comparer avec $\absfrac{\delta x}{x}$ et $\absfrac{\delta y}{y}$: on a donc une erreur multipliée par $\absfrac{x}{x+y}$ et $\absfrac{y}{x+y}$.

On a donc une erreur plus grand lorsque $x+y$ est proche de 0, c'est-à-dire que l'on additionne des nombres opposés.

\subsection{Erreur lors de la multiplication}

On calcule:
\begin{align*}
 \left|\frac{x y - (x+y+\delta x+\delta y)}{x y}\right| & = \left|\frac{x\delta y + y\delta x + \delta x\delta y}{x y}\right| \\
 & = \left|\frac{\delta y}y + \frac{\delta x}x + \frac{\delta x\delta y}{xy}\right|
\end{align*}
Au pire on somme les erreurs relatives de $x$ et $y$, donc il n'y a pas de problèmes de précision sur la multiplication.

\subsection{Erreur lors des calculs matriciels}

Soit $x\in \setR^n$ un vecteur. On note sa norme euclidienne $\norm{x}$ telle que:
\[ \norm{x} = \sqrt{\sum_{i=1}^n x_i^2} \]
En Python, on utilisera \lstinline{norm(x)}.

On définie la ``norme 1 de $x$'', qu'on note $\normo{x}$, telle que:
\[ \normo x = \sum_{i=1}^n \left|x\right| \]
En Python, on utilisera \lstinline{norm(x, 1)}.

Soit $A \in M_{m,n}(\setR)$ une matrice dans l'espace des matrices de $m$ lignes et $n$ colonnes.

\paragraph{Rappel}
Pour tout $x \in \setR^n$ et pour toute matrice $A \in M_{m, m}(\setR)$, on a l'inégalité $\norm{A_x} \leq \norm{A} \norm{x}$.

\paragraph{Conditionnement} On appelle conditionnement de $A$ (noté $\cond(A)$) la quantité:
\[
    \cond(A) =
    \begin{cases*}
        \norm{A} \norm{A^{-1}} & si la matrice est réversible ($A^{-1}$ existe) \\
        +\infty & sinon
    \end{cases*}
\]

Numériquement, il est rare d'obtenir $+\infty$. Si on obtient $10^{16}$ en double précision, il est probable que $A$ soit irréversible.

Si le conditionnement est petit mais supérieur à 1, on dit que la matrice est ``bien conditionnée''.

\paragraph{Exemple}

On veut résoudre le système $A_x = b$ avec $x, b \in \setR^n$ et $A \in M_{m,m}(\setR)$.

En pratique, on obtient une solution approchée $y = x + \delta x$ où $\delta x$ est petit. Cette situation ne vérifiera pas $A_y = b$ mais $A_y = b + \delta b$. $\delta b$ est facile à calculer: $\delta b = A_y - b$. On appelle $\norm{\delta b}$ le résultat. À partir de $\norm{\delta b}$, peut-on estimer $\norm{\delta x}$?

On part de $A(x+\delta x) = b + \delta b$ et il faut majorer $\normfrac{\delta x}{x}$ en fonction de $\normfrac{\delta b}{b}$ et $\cond(A)$. On a $A_x = b$.

\begin{align*}
  A(x+\delta x) &= b+\delta b \\
  A\delta x &= \delta b \\
  \delta x &= A^{-1} \delta b \\
  \norm{\delta x} &\leq \norm{A^{-1}} \norm{\delta b} \\
  \norm{b} &\leq \norm{A} \norm{x} \\
  \norm{\delta x} \norm{b} &\leq \cond(A) \norm{\delta b} \norm{x} \\
  \normfrac{\delta x}{x} &\leq \cond(A) \normfrac{\delta b}{b}
\end{align*}

Conclusion: si $A$ est mal conditionnée, il faut un résidut très faible pour obtenir un $y$ fiable.

\section{Principe pour minimiser les erreurs d'arrondi}

\subsection{Éviter de soustraire des quantités proches}

Par exemple, pour calculer les racines d'un polynôme de degré 2, avec:
\[a x^2 + b x + c = 0\]
\[\Delta = b^2 + 4 a c\]
\[x_1 = \frac{-b+\sqrt{\Delta}}{2 a} \text{ et } x_2 = \frac{-b-\sqrt{\Delta}}{2 a}\]

Puisque $\left|4 a c\right| \ll b^2$ on sait que $\sqrt{\Delta} \approx b$. Or, on fait $-b+\sqrt{\Delta}$ pour calculer $x_1$, ce qui est une grande perte de précision.

La solution est de calculer $x_2 = \frac{-b-\sqrt{\Delta}}{2 a}$, et de profiter du fait que $x_1 x_2 = \frac c a$ d'où $x_1 = \frac c {a x_2}$.

\subsection{Lors d'une somme, commencer par les termes les plus petits}

En commençant par les termes les plus grands, tous les petits termes n'apparaîtront pas à cause du nombre de chiffres significatifs; ils n'auront donc aucune influence sur le résultat et sont perdus.

Si l'on commence par les petits termes, ils s'accumulent avant l'arrondi lors de l'ajout des grands termes, ils ont donc une chance d'influencer le total collectivement.

\chapter{Résolution des systèmes linéaires}

\chapter{Calcul des éléments propres}

L'objectif est de trouver $\lambda \in \setC$ et $x \in \setC^n$ ($x \neq 0$) tel que $A x = \lambda x$. Si $A$ est réelle, les valeurs propres sont complexes conjuguées. $\lambda$ est appelée valeur propre (eigenvalue) et $x$ est appelé vecteur propre.

\[ Sp(A) = \lbrace \lambda \text{ tel que } \lambda \text{ valeur propre de } A \rbrace = \text{spectre de } A \]

\paragraph{Diagonalisation}
On dit que $A$ est diagonalisable si et seulement si elle est semblable à une matrice diagonale, c'est-à-dire qu'il existe $P$ matrice irréversible et $D$ diagonale telle que $A = P D P^{-1}$. $P$ est appelée ``matrice de passage''.

L'ensemble des matrices diagonalisables est ``dense'' (on peut toujours trouver une suite $A_k$ de matrices diagonalisables telles que $\lim A_k = A$).

En Python, pour calculer $D$ et $P$, on utilise \lstinline{eig}:
\begin{lstlisting}
D, P = eig(A)
\end{lstlisting}
les colonnes de $P$ sont les vecteurs propres de $A$. Si $A$ n'est pas diagonalisable, $P$ ne sera pas inversible ($\cond(P) \approx 10^{16}$).

La notion de valeur propre s'applique qu'aux matrices carrées. Pour les matrices rectangulaires, on a la notion de décomposition en valeurs singulières: $A$ peut s'écrire sous la forme:
\[ A = U S V^{T} \]
avec $U$ et $V$ matrices orthogonales et $S$ diagonale.

Rappel: $Q$ est orthogonale si et seulement si $Q Q^T = I$.

\paragraph{Transformation de Householder}
C'est une transformation orthogonale de symétrie par rapport au plan formé par deux vecteurs; c'est la symétrie par rapport au plan médian.

La normale du plan: \[\overrightarrow{n} = \frac{X-Y}{\norm{X-Y}}\] où $X \in \setR^n$, $Y \in \setR^n$ et $\norm x = \norm y$.

Si $X = Y$, la transformation de Householder est l'identité.

Sa transformation est notée $H$ et vaut:
\[ H(\overrightarrow{u}) = \overrightarrow{u} - 2 (\overrightarrow{u} \cdot \overrightarrow{n}) \overrightarrow{n} \]

La matrice associée à $H$ s'écrit $\mathcal{H} = I - 2 N^T$ (où $N^T$ est une matrice de termes $n_i n_j$).

Pour calculer $N N^T$ en Python, on fait \lstinline{outer(N, N)}.

Stocker la matrice $\mathcal{H}$ a peu d'intérêt car il est plus efficace de calculer \[ \mathcal{H} \overrightarrow{u} = H(\overrightarrow u) = \overrightarrow{u} - 2 (\overrightarrow{n} \cdot \overrightarrow{u}) \overrightarrow{n} \] (qui se calcule en $O(n)$) alors que $\mathcal H U$ (qui se calcule en $O(n^2)$).

Pour effectuer le produit matrice matrice $\mathcal H A$ avec $A$ une matrice, on applique $H(\overrightarrow u)$ pour toutes les colonnes $\overrightarrow u$ de $A$: la complexité est en $O(n^2)$.

Pour calculer $A \mathcal H$ on applique $H$ aux lignes de $A$.

\paragraph{Factorisation QR}
On cherche $Q$ une matrice orthogonale et $R$ une matrice triangulaire telles que $A = Q R$ ($A$ peut être rectangulaire).

En Python, on écrit:
\begin{lstlisting}
Q, R = qr(A)
\end{lstlisting}

Un algorithme possible est:
\begin{lstlisting}
R = A
Q = I
for i les indices des colonnes de A:
    X = R[i:, i] # i: tous les indices à partir de i
                   #  = (i, i+1, i+2, ... n-1)
                   # X: fin de colonne i de R
    alpha = -norm(X) * signe(X[0])
    Y = [alpha, 0, 0, 0, 0, 0, 0, 0...]
    # on calcule H la transformation de Householder de X à Y
    # on applique H à R[i:, i][:, i:]
    # R est écrasée avec les nouvelles valeurs
\end{lstlisting}
on a calculé: $Q_n Q_{n-1} \dots Q_1 A = R$ où $Q_i$ sont des matrices de Householder.

On a donc: \[ A = Q_1^T Q_2^T \dots Q_n^T R = Q_1 Q_2 \dots Q_n R \]

On peut utiliser la factorisation QR pour résoudre un système:
\begin{align*}
 A x = b &\Leftrightarrow Q R x = b\\
    &\Leftrightarrow Q^T Q R x = Q^T b\\
    &\Leftrightarrow R x = Q^T b
\end{align*}
mais cette méthode est plus coûteuse que la factorisation de Cholesky. Elle permet de minimiser $\norm{A x - b}$ quand $A$ est rectangulaire.

\paragraph{Exemple de factorisation QR}
Si on a les points $(x_i, y_i)_{i \leq i \leq m}$, on cherche $P(x) = a_0 + a_1 x + \dots + a_{n-1} x^{n-1}$. $P$ est un polynôme de degré $n-1$ tel que la quantité $\sum_{i=1}^m (y_i - P(x_i))^2$ soit minimale. On commence par trouver la matrice $A$ est le vecteur $b$ tels que $\sum_{i=1}^m (y_i - P(x_i))^2 = \norm{A z - b}^2$, puis on expliquera comment trouver $z$ tel que $\norm{A x - b}^2$ soit minimal en utilisant la factorisation QR.

Pour minimiser $\norm{A x - b}$ pour $A$ rectangulaire: \[ \norm x = \sqrt{\sum x_i^2} \]

\section{Calcul des valeurs propres}

Si $Q$ est une matrice orthogonale, alorsr $A$ et $Q A Q^T$ sont semblables (ie. même valeurs propres). L'idée est d'appliquer des transformations orthogonales successives $A_{k+1} = Q_k A_k Q_k^T$ telles que $A_k$ converge vers une matrice diagonale.

Dans le cas de la SVD, on construit une suite de matrices $Q_{l, k}$ eet $Q_{k, k}$ orthogonales telles que $A_{k+1} = Q_{i, k} A_k Q_{k, k}$ et telle que $\lim_{k \to +\infty} A_k =$ matrice diagonale.
\[A_n = Q_{L, N} Q_{L, N-1} \dots Q_{L, 1} A Q_{K, 1} Q_{K, 2} \dots Q_{R, N}\] d'où \[A = USV\] avec $U$ et $V$ orthogonales et $S$ diagonale.

\end{document}
